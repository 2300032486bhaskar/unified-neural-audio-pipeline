ğŸ§ Unified Neural Audio Pipeline
Speaker Extraction â€¢ Speech Recognition â€¢ Audio Source Separation

This project implements an end-to-end neural audio processing pipeline capable of:

âœ” Separating background music & vocals from any audio (Demucs)
âœ” Extracting a target speakerâ€™s voice from a mixture using speaker embedding
âœ” Generating transcription using Whisper ASR (faster-whisper)
âœ” Providing a simple FastAPI backend to upload & process audio files

This pipeline is lightweight, easy to customize, and built from scratch for demonstrations, academic reviews, and technical interviews.

ğŸš€ Features
ğŸ”¹ 1. Audio Source Separation (Demucs)

Separates input audio into:

Vocals

Drums

Bass

Other

ğŸ”¹ 2. Speaker Embedding + Target Speaker Extraction

Uses SpeechBrain ECAPA-TDNN to:

Generate speaker embeddings

Match similarity

Extract only the target speakerâ€™s voice from the mixture

ğŸ”¹ 3. Speech Recognition (Whisper ASR)

Uses Faster-Whisper small model for transcription.

ğŸ”¹ 4. FastAPI Backend

Simple /submit-offline/ endpoint to upload:

mixture_audio.wav

target_speaker_sample.wav

Returns:

Extracted target speaker audio

JSON diarization/transcription metadata

ğŸ—‚ï¸ Project Structure
unified-neural-pipeline/
â”‚
â”œâ”€â”€ outputs/                # pipeline results (auto-created)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py              # FastAPI server
â”‚   â”œâ”€â”€ pipeline.py         # full processing pipeline
â”‚   â”œâ”€â”€ separation.py       # audio separation (Demucs)
â”‚   â”œâ”€â”€ asr_module.py       # Whisper ASR wrapper
â”‚   â”œâ”€â”€ speaker_embedding.py# Speaker embedding (ECAPA)
â”‚   â”œâ”€â”€ utils.py            # helper utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚
â”œâ”€â”€ requirements.txt        # dependencies
â”œâ”€â”€ README.md               # documentation
â”œâ”€â”€ run.py                  # one-line runner

ğŸ”§ Installation
1ï¸âƒ£ Create a virtual environment
python -m venv .venv

2ï¸âƒ£ Activate it

Windows:

.venv\Scripts\activate

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

â–¶ï¸ Running the Server

Start the FastAPI backend:

python src/app.py


You will see:

Uvicorn running on http://0.0.0.0:8000


Open the browser:
ğŸ‘‰ http://localhost:8000/docs

Upload the files and run the pipeline.

ğŸ“¤ API Usage
POST /submit-offline/

Upload two audio files:

Parameter	Type	Description
mixture	file (.wav)	Audio containing mixture (music + speech)
target	file (.wav)	3â€“5 second sample of target speaker
âœ” Sample Response
{
  "output_target": "outputs/target_speaker.wav",
  "diarization_output": "outputs/diarization.json"
}

ğŸ§  Models Used
Task	Model
Source Separation	Demucs v4
Speaker Embedding	ECAPA-TDNN (SpeechBrain)
Speech Recognition	Whisper-Small (Faster-Whisper)
ğŸ“Œ Notes

Works fully offline after the first model download.

Designed as an interview + academic review-friendly project.

Clean modular codebase for easy expansion.

ğŸ Conclusion

This project demonstrates a full neural audio pipeline integrating:

Source separation

Speaker extraction

Speech recognition

Backend service